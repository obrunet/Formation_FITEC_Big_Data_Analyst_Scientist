{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration related to Cassandra connector & Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set some arguments or configurations to make sure PySpark connects to our Cassandra node cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# credits : https://medium.com/coinmonks/running-pyspark-with-cassandra-using-spark-cassandra-connector-in-jupyter-notebook-9f1dc45e8dc9\n",
    "# edit spark.cassandra.connection.host= NAME_OF_THE_CONTAINER or the local IP\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.1 --conf spark.cassandra.connection.host=cassandra pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are saying that use spark-cassandra-connector to connect to our Cassandra cluster and its hostname is 127.0.0.1 OR THE NAME YOU GIVE IN THE DOCKER COMPOSE YAML FILE. That’s it. For example, you have multiple nodes in your Cassandra cluster then in the host configuration, we need to give all of their ips. For example,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create the Spark Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"cours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we need to create SQL Context to do SQL operations on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on multiple tables so need their data frames to save some lines of code created a function which loads data frame for a table including key space given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load the data, and see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------------+-----------+------+-----------+\n",
      "|idcours|coeff|            intitule|nbheuresmax|niveau|responsable|\n",
      "+-------+-----+--------------------+-----------+------+-----------+\n",
      "|      1|    3|Introduction aux ...|         30|    M1|          1|\n",
      "+-------+-----+--------------------+-----------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = load_and_get_table_df(\"ecole\", \"cours\")\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
